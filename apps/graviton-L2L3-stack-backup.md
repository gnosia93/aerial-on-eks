## ê·¸ë¼ë¹„í†¤ ê¸°ë°˜ L2/L3 ìŠ¤íƒ êµ¬í˜„ ##

L2/L3 ìŠ¤íƒì€ ìˆ˜í•™ì  í–‰ë ¬ ì—°ì‚°ë³´ë‹¤ëŠ” íŒ¨í‚·ì˜ ìˆœì„œë¥¼ ë§ì¶”ê³ (RLC), ì‚¬ìš©ìë³„ë¡œ ìì›ì„ í• ë‹¹(MAC Scheduler)í•˜ë©°, IP íŒ¨í‚·ì„ ìº¡ìŠí™”í•˜ëŠ” ë³µì¡í•œ ë…¼ë¦¬ ì²˜ë¦¬ê°€ ì£¼ë¥¼ ì´ë£¬ë‹¤. ì•„ë˜ëŠ” Graviton(ARM64) ë…¸ë“œì—ì„œ ì‹¤í–‰í•˜ê¸° ì í•©í•œ íŒŒì´ì¬ ê¸°ë°˜ì˜ ë‹¨ìˆœí™”ëœ L2(MAC) ìŠ¤ì¼€ì¤„ëŸ¬ ë° ë°ì´í„° ì²˜ë¦¬ ìƒ˜í”Œ ì½”ë“œë¡œ TX Podë¡œë¶€í„° ë°›ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ RX Podë¡œ ì „ë‹¬í•˜ëŠ” "ì¤‘ê³„ ë° ì œì–´" ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.
```
import numpy as np
import time
from concurrent.futures import ThreadPoolExecutor

class L2L3Stack:
    def __init__(self):
        # Gravitonì˜ ë©€í‹°ì½”ì–´ë¥¼ í™œìš©í•˜ê¸° ìœ„í•œ ì„¤ì •
        self.executor = ThreadPoolExecutor(max_workers=8) 
        print("[L2/L3] Graviton-based Protocol Stack Initialized (ARM64)")

    def mac_scheduler(self, data):
        """
        L2 ì—­í• : ë°ì´í„°ë¥¼ ì‚¬ìš©ìë³„/ìš°ì„ ìˆœìœ„ë³„ë¡œ ìŠ¤ì¼€ì¤„ë§
        """
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ë³µì¡í•œ Priority Queue ì²˜ë¦¬ê°€ ì¼ì–´ë‚¨
        processed_data = data * 1.0  # ë‹¨ìˆœ íŒ¨í‚· í†µê³¼ ì‹œë®¬ë ˆì´ì…˜
        return processed_data

    def rlc_layer_processing(self, packet_id, data):
        """
        L2 ì—­í• : íŒ¨í‚· ë²ˆí˜¸ ë¶€ì—¬ ë° ì¬ì „ì†¡ ì œì–´(ARQ) ë¡œì§
        """
        # Gravitonì˜ ì •ìˆ˜ ì—°ì‚° ì„±ëŠ¥ì„ í™œìš©í•œ í—¤ë” ë¶€ì°© ë° ë¬´ê²°ì„± ê²€ì‚¬
        header = f"SN-{packet_id}".encode()
        return header + data.tobytes()

    def process_pipeline(self, packet_id, raw_signal):
        # 1. MAC ê³„ì¸µ ìŠ¤ì¼€ì¤„ë§
        scheduled_data = self.mac_scheduler(raw_signal)
        # 2. RLC/PDCP ê³„ì¸µ ì²˜ë¦¬ (ë°”ì´ë„ˆë¦¬ ì¸ì½”ë”©)
        final_packet = self.rlc_layer_processing(packet_id, scheduled_data)
        return final_packet

l2l3 = L2L3Stack()
packet_count = 0

try:
    while True:
        # ê°€ìƒì˜ TX Podë¡œë¶€í„° ë°›ì€ IQ ìƒ˜í”Œ ë°ì´í„° (Sionna ê²°ê³¼ë¬¼)
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” gRPCë‚˜ Shared Memoryë¥¼ í†µí•´ ìˆ˜ì‹ í•©ë‹ˆë‹¤.
        mock_iq_samples = np.random.standard_normal(1024).astype(np.complex64)
        
        # Graviton ë©€í‹°ì½”ì–´ë¥¼ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        future = l2l3.executor.submit(l2l3.process_pipeline, packet_count, mock_iq_samples)
        result = future.result()
        
        if packet_count % 100 == 0:
            print(f"[L2/L3] Packet {packet_count} processed on ARM64 core")
            
        packet_count += 1
        time.sleep(0.01) # ì²˜ë¦¬ ì£¼ê¸° ì¡°ì ˆ
except KeyboardInterrupt:
    print("Stack stopped.")
```

* ë°ì´í„° ì§ë ¬í™”: TX(x86)ì—ì„œ L2/L3(ARM)ë¡œ ë°ì´í„°ë¥¼ ë³´ë‚¼ ë•Œ, ì—”ë””ì•ˆ(Endian) ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ Protobufë‚˜ Pickle ë“±ì„ ì‚¬ìš©í•´ ì§ë ¬í™”í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
* ì´ êµ¬ì¡°ì—ì„œ Graviton(L2/L3)ì€ ì§ì ‘ì ì¸ ë¬¼ë¦¬ ì‹ í˜¸(IQ ìƒ˜í”Œ)ë¥¼ ê±´ë“œë¦¬ê¸°ë³´ë‹¤, ë°ì´í„°ë¥¼ 'ìº¡ìŠí™”(Encapsulation)'í•˜ì—¬ ìˆ˜ì‹ ì¸¡ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ë°ì´í„° ê²Œì´íŠ¸ì›¨ì´ ì—­í• ì„ í•œë‹¤.
ì‹¤ì œ ì‹œìŠ¤í…œì—ì„œëŠ” í¬ê²Œ ë‘ ê°€ì§€ ì—°ê²° ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

#### 1. ë„¤íŠ¸ì›Œí¬ í†µì‹  ë°©ì‹ (gRPC ë˜ëŠ” UDP) ####
ê°€ì¥ ë³´í¸ì ì¸ ë°©ì‹ìœ¼ë¡œ, TX/L2L3/RXê°€ ê°ê° ë³„ë„ì˜ Podë¡œ ë–  ìˆì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
* íë¦„: TX (x86) â†’ gRPC ì „ì†¡ â†’ L2/L3 (Graviton) â†’ gRPC ì „ì†¡ â†’ RX (x86)
* Gravitonì˜ ì—­í• : TXì—ì„œ ë°›ì€ IQ ë°ì´í„°ë¥¼ Protobuf(gRPC) ë©”ì‹œì§€ì— ë‹´ê³ , í—¤ë”(Sequence Number ë“±)ë¥¼ ë¶™ì—¬ RX Podì˜ IP ì£¼ì†Œë¡œ ì´ì¤€ë‹¤.

  

### Gravitonì„ ê±°ì³ RXë¡œ ê°€ëŠ” í˜„ì‹¤ì ì¸ ë°ì´í„° íë¦„ ###
GPU ë…¸ë“œì™€ Graviton ë…¸ë“œ ì‚¬ì´ì˜ í†µì‹ ì€ ë‹¤ìŒ 3ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
* Serialization (ì§ë ¬í™”): TX(x86)ì—ì„œ ìƒì„±ëœ complex64 ë°ì´í„°ë¥¼ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
* L2/L3 Processing (Graviton): ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· í˜•íƒœë¡œ ë“¤ì–´ì˜¨ ë°ì´í„°ë¥¼ Gravitonì´ ìˆ˜ì‹ í•˜ì—¬ í”„ë¡œí† ì½œ ì²˜ë¦¬(í—¤ë” ê²€ì‚¬, ìŠ¤ì¼€ì¤„ë§ ê²°ì • ë“±)ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* Forwarding (í¬ì›Œë”©): ì²˜ë¦¬ê°€ ëë‚œ íŒ¨í‚·ì„ RX(x86)ì˜ IPë¡œ ì´ì¤ë‹ˆë‹¤.

ğŸš€ ì„±ëŠ¥ ìµœì í™” ëŒ€ì•ˆ: gRPC ëŒ€ì‹  'UDP Raw Socket'
* Sionna ì‹¤ì‹œê°„ ì‹ í˜¸ ì „ì†¡ì²˜ëŸ¼ ë°ì´í„° ì–‘ì´ ë§ì„ ë•ŒëŠ” TCP ê¸°ë°˜ì˜ gRPCë³´ë‹¤ UDPê°€ ìœ ë¦¬í•©ë‹ˆë‹¤. íŠ¹íˆ Graviton ë…¸ë“œì—ì„œ ê³ ì„±ëŠ¥ ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤(ENA)ë¥¼ í™œìš©í•˜ë©´ íŒ¨í‚· ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©° ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
# 1. TX Pod (Sionna - x86 + GPU)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sionna-tx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tx-node
  template:
    metadata:
      labels:
        app: tx-node
    spec:
      nodeSelector:
        kubernetes.io/arch: amd64 # x86 ë…¸ë“œ ê°•ì œ
        nvidia.com: "true"
      containers:
      - name: tx-container
        image: <your-x86-sionna-image>
        env:
        - name: L2L3_ENDPOINT
          value: "l2l3-service:5000" # Graviton ì„œë¹„ìŠ¤ ì£¼ì†Œ
        resources:
          limits:
            nvidia.com: 1
---
# 2. L2/L3 ìŠ¤íƒ Pod (Graviton - ARM64)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: l2l3-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: l2l3-node
  template:
    metadata:
      labels:
        app: l2l3-node
    spec:
      nodeSelector:
        kubernetes.io/arch: arm64 # Graviton ë…¸ë“œ ê°•ì œ
      containers:
      - name: l2l3-container
        image: <your-arm64-l2l3-image>
        ports:
        - containerPort: 5000 # TXë¡œë¶€í„° ë°ì´í„° ë°›ëŠ” í¬íŠ¸
---
# 3. L2/L3 ì„œë¹„ìŠ¤ (ë‚´ë¶€ í†µì‹ ìš©)
apiVersion: v1
kind: Service
metadata:
  name: l2l3-service
spec:
  selector:
    app: l2l3-node
  ports:
  - protocol: UDP
    port: 5000
    targetPort: 5000
```

#### 2. Pod ê°„ í†µì‹  ì„¤ê³„ (UDP/Binary ì „ì†¡) ###
ì•„í‚¤í…ì²˜(x86 â†” ARM)ê°€ ë‹¤ë¥´ë¯€ë¡œ ë°”ì´ë„ˆë¦¬ ì—”ë””ì•ˆ(Little-Endian)ì„ ë§ì¶˜ UDP ì „ì†¡ì´ ê°€ì¥ ë¹ ë¦…ë‹ˆë‹¤.

[TX ìª½: Sionna ì‹ í˜¸ ì†¡ì‹ ]
```
import socket
import numpy as np

# UDP ì„¤ì •
L2L3_ADDR = ("l2l3-service", 5000)
sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

def send_signal(iq_samples):
    # complex64 ë°ì´í„°ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ì§ë ¬í™” (Little-endian ë³´ì¥)
    payload = iq_samples.tobytes() 
    sock.sendto(payload, L2L3_ADDR)
```

[Graviton ìª½: L2/L3 ë°ì´í„° ì¤‘ê³„]
```
import socket
import numpy as np

sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
sock.bind(('0.0.0.0', 5000))

print("[Graviton] L2/L3 Stack Listening...")

while True:
    data, addr = sock.recvfrom(65535) # ìµœëŒ€ MTU ê³ ë ¤
    # 1. ìˆ˜ì‹  ë°ì´í„°ë¥¼ numpyë¡œ ë³µì› (ARMì—ì„œ ì²˜ë¦¬)
    iq_samples = np.frombuffer(data, dtype=np.complex64)
    
    # 2. L2/L3 ì œì–´ ë¡œì§ (Scheduling ë“±) ìˆ˜í–‰
    # ... logic ...
    
    # 3. RX Pod(x86)ë¡œ ë‹¤ì‹œ í† ìŠ¤
    sock.sendto(data, ("rx-service", 5001))
```
ì›Œí¬ìƒµ ì„±ê³µì„ ìœ„í•œ íŒ
* ì´ë¯¸ì§€ ë¹Œë“œ: docker buildxë¥¼ ì‚¬ìš©í•˜ì—¬ linux/amd64(TX/RXìš©)ì™€ linux/arm64(L2L3ìš©) ì´ë¯¸ì§€ë¥¼ ê°ê° ë¹Œë“œí•˜ì—¬ Amazon ECRì— ë°€ì–´ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.
ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì¶”ì²œ:
* TX/RX: g5.xlarge (NVIDIA A10G)
* L2/L3: c7g.medium ë˜ëŠ” c7gn.medium (ë„¤íŠ¸ì›Œí¬ ìµœì í™”í˜• Graviton)
* ë„¤íŠ¸ì›Œí¬ ë³‘ëª©: ë°ì´í„° ì–‘ì´ ë„ˆë¬´ ë§ìœ¼ë©´ SOCK_DGRAM ëŒ€ì‹  NVIDIA DPDKë¥¼ ê³ ë ¤í•´ì•¼ í•˜ì§€ë§Œ, ì›Œí¬ìƒµ ìˆ˜ì¤€ì—ì„œëŠ” UDPë§Œìœ¼ë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤.
ì´ë ‡ê²Œ êµ¬ì„±í•˜ë©´ "AI/ì‹ í˜¸ì²˜ë¦¬ëŠ” GPU(x86)ê°€, í†µì‹  í”„ë¡œí† ì½œì€ ê°€ì„±ë¹„ì˜ Graviton(ARM)ì´" ë‹´ë‹¹í•˜ëŠ” ìµœì‹ í˜• O-RAN ì‹œë®¬ë ˆì´ì…˜ êµ¬ì¡°ê°€ ì™„ì„±ë©ë‹ˆë‹¤.

----
RX(ìˆ˜ì‹ ) PodëŠ” Graviton(L2/L3)ìœ¼ë¡œë¶€í„° ì „ë‹¬ë°›ì€ ë°”ì´ë„ˆë¦¬ íŒ¨í‚·ì„ ë‹¤ì‹œ Sionna/TensorFlowê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë³µì†Œìˆ˜ í…ì„œë¡œ ë³µì›í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ ì‹ í˜¸ë¥¼ ë³µì¡°(Demodulation)í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
x86 í™˜ê²½ì—ì„œ NVIDIA GPU ê°€ì†ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë³µì›í•˜ëŠ” í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤.

```
import socket
import numpy as np
import tensorflow as tf
from sionna.utils import QAMSource, BinarySource
# Sionnaì˜ Mapper/Demapper ì„¤ì • (TXì™€ íŒŒë¼ë¯¸í„° ë™ì¼í•´ì•¼ í•¨)
from sionna.mapping import Mapper, Demapper

class SignalReceiver(tf.keras.Model):
    def __init__(self, num_bits_per_symbol=4):
        super().__init__()
        self.demapper = Demapper("app", constellation_type="qam", num_bits_per_symbol=num_bits_per_symbol)
        # ë¹„êµë¥¼ ìœ„í•œ ì†ŒìŠ¤ (BER ê³„ì‚°ìš©)
        self.binary_source = BinarySource()

    def decode_signal(self, iq_samples_raw, ebno_db=20.0):
        # 1. ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ complex64ë¡œ ë³€í™˜
        iq_samples = np.frombuffer(iq_samples_raw, dtype=np.complex64)
        
        # 2. í…ì„œë¡œ ë³€í™˜ ë° GPUë¡œ ì „ì†¡
        y = tf.convert_to_tensor(iq_samples)
        y = tf.reshape(y, [64, -1]) # TXì—ì„œ ë³´ë‚¸ Batch sizeì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì¤Œ
        
        # 3. ë…¸ì´ì¦ˆ ì¶”ì •ì¹˜ ì„¤ì • (AWGN ê¸°ì¤€)
        no = tf.cast(10**(-ebno_db/10), tf.float32)
        
        # 4. Demapping (IQ ìƒ˜í”Œ -> LLR/Bits)
        llr = self.demapper([y, no])
        
        # 5. ìµœì¢… ë¹„íŠ¸ ê²°ì • (Hard Decision)
        bits_hat = tf.cast(llr < 0, tf.int32)
        return bits_hat

# --- RX Pod ì‹¤í–‰ë¶€ ---
rx = SignalReceiver()
sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
sock.bind(('0.0.0.0', 5001)) # L2/L3(Graviton)ì´ ì´ì£¼ëŠ” í¬íŠ¸

print("[RX] Receiver is ready (x86 + GPU)")

while True:
    # 1. Gravitonìœ¼ë¡œë¶€í„° ë°ì´í„° ìˆ˜ì‹ 
    data, addr = sock.recvfrom(65535) 
    
    # 2. ì‹ í˜¸ ë³µì› ë° ë¹„íŠ¸ ì¶”ì¶œ
    with tf.device('/GPU:0'): # GPU ê°€ì† ê°•ì œ
        recovered_bits = rx.decode_signal(data)
    
    # 3. ê²°ê³¼ ì¶œë ¥ (ìƒ˜í”Œ)
    print(f"[RX] Recovered Shape: {recovered_bits.shape} on GPU")

```
êµ¬ì¡°ì  í•µì‹¬ í¬ì¸íŠ¸
* ë°ì´í„° ë¬´ê²°ì„±: TX â†’ Graviton â†’ RXë¥¼ ê±°ì¹˜ë©´ì„œ ë°ì´í„°ê°€ ê¹¨ì§€ì§€ ì•Šë„ë¡ np.frombuffer í˜¸ì¶œ ì‹œ ë°˜ë“œì‹œ dtype=np.complex64ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. NVIDIA SionnaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ float32 ê¸°ë°˜ì˜ ë³µì†Œìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
* GPU ì»¨í…ìŠ¤íŠ¸: RX PodëŠ” ìˆ˜ì‹  ëŒ€ê¸° ìƒíƒœ(recvfrom)ì—ì„œ CPUë¥¼ ì“°ë‹¤ê°€, ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ëŠ” ì¦‰ì‹œ tf.device('/GPU:0')ë¥¼ í†µí•´ NVIDIA GPUë¡œ ì—°ì‚°ì„ ë°€ì–´ ë„£ì–´ ì²˜ë¦¬ ì§€ì—°ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
* ë³‘ëª© í˜„ìƒ ë°©ì§€: UDP íŒ¨í‚· í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì¦ì€ GPU ì „ì†¡ ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•©ë‹ˆë‹¤. Gravitonì—ì„œ íŒ¨í‚·ì„ ì–´ëŠ ì •ë„ ëª¨ì•„ì„œ ë³´ë‚´ê±°ë‚˜(Batching), Shared Memoryê°€ ë¶ˆê°€ëŠ¥í•œ í™˜ê²½ì´ë¯€ë¡œ Jumbo Frame ì„¤ì •ì„ EKS ë…¸ë“œì— ì ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
* ì´ì œ ì „ì²´ íŒŒì´í”„ë¼ì¸(TX-Sionna â†’ L2/L3-Graviton â†’ RX-PyAerial)ì´ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ í™˜ê²½ì„ í•œ ë²ˆì— ë°°í¬í•  ìˆ˜ ìˆëŠ” Helm Chart êµ¬ì¡°ë¥¼ ì¡ì•„ë“œë¦´ê¹Œìš”? ì•„ë‹ˆë©´ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§(BER ì¸¡ì •) ë¡œì§ì„ ì¶”ê°€í•´ ë³¼ê¹Œìš”?
----

ì›Œí¬ìƒµì˜ ì™„ì„±ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´, ë©€í‹° ì•„í‚¤í…ì²˜ ë…¸ë“œ ë°°ì¹˜(Node Affinity)ë¥¼ ì§€ì›í•˜ëŠ” Helm Chartì™€ ì‹¤ì‹œê°„ìœ¼ë¡œ í†µì‹  í’ˆì§ˆì„ ì¸¡ì •í•˜ëŠ” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§(BER/Throughput) ë¡œì§ì„ í†µí•©í•´ ë“œë¦½ë‹ˆë‹¤.

#### 1. Helm Chart êµ¬ì¡° (Sionna Multi-Arch) ####
values.yamlì—ì„œ ì•„í‚¤í…ì²˜ë³„ ì„¤ì •ì„ ê´€ë¦¬í•˜ê³ , ê° í…œí”Œë¦¿ì—ì„œ ì´ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.
[values.yaml]
```
global:
  domain: "sionna.workshop"

tx:
  image: "your-repo/sionna-tx-x86:latest"
  gpuLimit: 1
  ebno_db: 20.0

l2l3: # Graviton ì „ìš©
  image: "your-repo/l2l3-stack-arm64:latest"
  replicaCount: 2

rx:
  image: "your-repo/sionna-rx-x86:latest"
  gpuLimit: 1
```

[templates/tx-deployment.yaml (GPU/x86 ë…¸ë“œ ê°•ì œ)]
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Release.Name }}-tx
spec:
  template:
    spec:
      nodeSelector:
        kubernetes.io/arch: amd64
        nvidia.com: "true"
      containers:
      - name: tx
        image: {{ .Values.tx.image }}
        resources:
          limits:
            nvidia.com: {{ .Values.tx.gpuLimit }}

```

## 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¡œì§ (RX ì¸¡ BER ì¸¡ì •) ##
ìˆ˜ì‹ ë‹¨(RX)ì—ì„œ ì†¡ì‹  ë°ì´í„°ì™€ ëŒ€ì¡°í•˜ì—¬ ë¹„íŠ¸ ì—ëŸ¬ìœ¨(BER)ì„ ê³„ì‚°í•˜ê³  ì´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•˜ê±°ë‚˜ ì‹œê°í™” íˆ´ë¡œ ë³´ë‚¼ ìˆ˜ ìˆëŠ” ì½”ë“œì…ë‹ˆë‹¤.

rx_logic.py
```
import tensorflow as tf
import numpy as np
from sionna.utils import BinarySource

class PerformanceMonitor:
    def __init__(self):
        print("[Monitor] Performance Tracker Initialized")

    def calculate_ber(self, original_bits, recovered_bits):
        """
        ë¹„íŠ¸ ì—ëŸ¬ìœ¨(Bit Error Rate) ê³„ì‚°
        """
        # 0ê³¼ 1ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ì—ëŸ¬ ê°œìˆ˜ íŒŒì•…
        error_count = tf.reduce_sum(tf.abs(original_bits - recovered_bits))
        total_bits = tf.cast(tf.size(original_bits), tf.float32)
        ber = error_count / total_bits
        return ber.numpy()

# RX ë£¨í”„ ë‚´ ì ìš© ì˜ˆì‹œ
monitor = PerformanceMonitor()
source = BinarySource()

while True:
    data, addr = sock.recvfrom(65535)
    
    # 1. ìˆ˜ì‹  ë° ë³µì¡° (ì´ì „ ë‹¨ê³„ ì½”ë“œ ì°¸ì¡°)
    recovered_bits = rx.decode_signal(data)
    
    # 2. ì„±ëŠ¥ í‰ê°€ (ì‹¤ì œ í™˜ê²½ì„  TXê°€ ë³´ë‚¸ ì‹œí€€ìŠ¤ ë„˜ë²„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì¡°)
    # ì—¬ê¸°ì„œëŠ” ë°ëª¨ë¥¼ ìœ„í•´ ë™ì¼ í¬ê¸°ì˜ ëœë¤ ì†ŒìŠ¤ì™€ ë¹„êµ ì˜ˆì‹œ
    mock_original = source([64, 1024]) 
    current_ber = monitor.calculate_ber(mock_original, recovered_bits)
    
    print(f"ğŸ“Š [Real-time Stats] BER: {current_ber:.6f} | Status: {'Stable' if current_ber < 0.01 else 'High Error'}")

```

### ì „ì²´ ì•„í‚¤í…ì²˜ ì›Œí¬í”Œë¡œìš° ìš”ì•½ ###
* AWS EKS í´ëŸ¬ìŠ¤í„°ì— x86 GPU ë…¸ë“œê·¸ë£¹ê³¼ Graviton ARM ë…¸ë“œê·¸ë£¹ì„ ê°ê° ìƒì„±í•©ë‹ˆë‹¤.
* Helmì„ í†µí•´ helm install sionna-workshop ./chart ëª…ë ¹ì–´ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ë°°í¬í•©ë‹ˆë‹¤.
* TX Podê°€ NVIDIA TensorFlow í™˜ê²½ì—ì„œ ì‹ í˜¸ë¥¼ ìƒì„±í•´ Graviton L2/L3 Podë¡œ ì©ë‹ˆë‹¤.
* Gravitonì€ ì €ë ´í•œ ë¹„ìš©ìœ¼ë¡œ íŒ¨í‚·ì„ ì¤‘ê³„/ìŠ¤ì¼€ì¤„ë§í•˜ê³  RX Podë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
* RX PodëŠ” ë‹¤ì‹œ GPU ê°€ì†ì„ í†µí•´ ì‹ í˜¸ë¥¼ ë³µì›í•˜ê³  BER ìˆ˜ì¹˜ë¥¼ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ(ì˜ˆ: Prometheus/Grafana)ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

----

EKS í™˜ê²½ì—ì„œ Graviton(L2/L3)ê³¼ GPU(TX/RX) ë…¸ë“œ ì‚¬ì´ì˜ ì„±ëŠ¥ ì§€í‘œ(BER, Throughput)ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´, Prometheus Python Clientë¥¼ í™œìš©í•œ ë©”íŠ¸ë¦­ ë…¸ì¶œ ë° ServiceMonitor ì„¤ì • ë°©ë²•ì„ ì•ˆë‚´í•´ ë“œë¦½ë‹ˆë‹¤.

#### 1. RX Podì— Prometheus ë©”íŠ¸ë¦­ ë…¸ì¶œ ì½”ë“œ ì¶”ê°€ ####
ìˆ˜ì‹ ë‹¨(RX)ì—ì„œ ê³„ì‚°ëœ BER(ë¹„íŠ¸ ì—ëŸ¬ìœ¨)ê³¼ ì²˜ë¦¬ëŸ‰(Throughput)ì„ Prometheusê°€ ê¸ì–´ê°ˆ ìˆ˜ ìˆë„ë¡ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì—´ì–´ì¤ë‹ˆë‹¤
```
from prometheus_client import start_http_server, Gauge, Counter
import time

# 1. Prometheus ë©”íŠ¸ë¦­ ì •ì˜
BER_GAUGE = Gauge('sionna_rx_ber', 'Current Bit Error Rate')
THROUGHPUT_COUNTER = Counter('sionna_rx_bits_total', 'Total bits processed')
LATENCY_GAUGE = Gauge('sionna_processing_latency_ms', 'End-to-end latency in ms')

# 2. ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘ (8000 í¬íŠ¸)
start_http_server(8000)

def monitor_performance(original_bits, recovered_bits, start_time):
    # BER ê³„ì‚° ë¡œì§
    error_count = np.sum(np.abs(original_bits - recovered_bits))
    total_bits = recovered_bits.size
    ber = error_count / total_bits
    
    # 3. Prometheusì— ê°’ ì—…ë°ì´íŠ¸
    BER_GAUGE.set(ber)
    THROUGHPUT_COUNTER.inc(total_bits)
    LATENCY_GAUGE.set((time.time() - start_time) * 1000)

```

#### 2. Prometheus ServiceMonitor ì„¤ì • (YAML) ####
Prometheus Operatorë¥¼ ì‚¬ìš© ì¤‘ì´ë¼ë©´, ì•„ë˜ ì„¤ì •ìœ¼ë¡œ ìë™ìœ¼ë¡œ RX Podì˜ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
```
apiVersion: monitoring.coreos.com
kind: ServiceMonitor
metadata:
  name: sionna-rx-monitor
  labels:
    release: prometheus # í—¬ë¦„ ì°¨íŠ¸ ë¦´ë¦¬ìŠ¤ ë¼ë²¨ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
spec:
  selector:
    matchLabels:
      app: rx-node # RX Podì˜ ë¼ë²¨
  endpoints:
  - port: metrics # RX Podì—ì„œ 8000ë²ˆ í¬íŠ¸ì— ì§€ì •í•œ ì´ë¦„
    interval: 5s   # 5ì´ˆë§ˆë‹¤ ì‹¤ì‹œê°„ ìˆ˜ì§‘

```
3. Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„± íŒ
* ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ Grafanaì—ì„œ ì‹œê°í™”í•  ë•Œ ë‹¤ìŒ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:
* ì‹¤ì‹œê°„ BER: sionna_rx_ber
* í‰ê·  Throughput (Mbps): rate(sionna_rx_bits_total[1m]) / 1000000
* Graviton-x86 ê°„ ì§€ì—° ì‹œê°„: sionna_processing_latency_ms

ğŸ’¡ ì•„í‚¤í…ì²˜ ê´€ì „ í¬ì¸íŠ¸
* Graviton ë…¸ë“œ: CloudWatch Agentë‚˜ Prometheusë¥¼ í†µí•´ CPU ì‚¬ìš©ë¥ ì„ ëª¨ë‹ˆí„°ë§í•˜ì—¬, L2/L3 ë¡œì§ì´ ARM ì½”ì–´ì— ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì‚°ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
* GPU ë…¸ë“œ: NVIDIA DCGM Exporterë¥¼ í•¨ê»˜ ë„ìš°ë©´ Sionna ì—°ì‚° ì‹œ GPU ì´ìš©ë¥ (Utilization)ê³¼ ë©”ëª¨ë¦¬ ì ìœ ìœ¨ì„ ë™ì‹œì— ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

----
* Grafana Alertingì„ ì‚¬ìš©í•˜ë©´ Sionna ì‹œë®¬ë ˆì´ì…˜ ì¤‘ ì±„ë„ í™˜ê²½ì´ ì•…í™”ë˜ê±°ë‚˜ ì½”ë”© ì„±ëŠ¥ì´ ë–¨ì–´ì ¸ BER(ë¹„íŠ¸ ì—ëŸ¬ìœ¨)ì´ ì„ê³„ì¹˜ë¥¼ ë„˜ì—ˆì„ ë•Œ ì¦‰ì‹œ ì•Œë¦¼(Slack, Email ë“±)ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
---

#### 1. L2/L3ê°€ ì—†ì–´ë„ ë˜ëŠ” ê²½ìš° (ë‹¨ìˆœ ë¬¼ë¦¬ ê³„ì¸µ ì—°êµ¬) ####
ë‹¨ìˆœíˆ "ì´ ì•ˆí…Œë‚˜ ì•Œê³ ë¦¬ì¦˜ì´ ì—ëŸ¬ë¥¼ ì–¼ë§ˆë‚˜ ì¤„ì´ëŠ”ê°€?" ë˜ëŠ” "AI ë””ëª¨ë“ˆë ˆì´í„° ì„±ëŠ¥ì´ ì¢‹ì€ê°€?"ë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ TXì—ì„œ RXë¡œ ë°”ë¡œ ì´ë„ ë©ë‹ˆë‹¤.
* êµ¬ì¡°: TX (Sionna) â†’ Network â†’ RX (Sionna/PyAerial)
* ì¥ì : êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ê³  ì§€ì—° ì‹œê°„ì´ ì§§ìŠµë‹ˆë‹¤. Sionna ê³µì‹ íŠœí† ë¦¬ì–¼ì˜ ëŒ€ë¶€ë¶„ì€ ì´ ë°©ì‹ì…ë‹ˆë‹¤.

#### 2. ê·¸ëŸ¼ì—ë„ L2/L3ë¥¼ ë„£ëŠ” ì´ìœ  (ì§„ì§œ í†µì‹  ì‹œìŠ¤í…œ ëª¨ì‚¬) ####
ì›Œí¬ìƒµì˜ ì£¼ì œê°€ "ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´ë§"ì´ë‚˜ "O-RAN êµ¬ì¡° ì´í•´"ë¼ë©´ L2/L3ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•„ìˆ˜ ì—­í• ì„ í•©ë‹ˆë‹¤.
* ìŠ¤ì¼€ì¤„ë§ (Resource Allocation): TXê°€ ì‹ í˜¸ë¥¼ ë§‰ ì˜ëŠ” ê²Œ ì•„ë‹ˆë¼, "ì§€ê¸ˆ 1ë²ˆ ìœ ì €ëŠ” ìƒíƒœê°€ ì¢‹ìœ¼ë‹ˆ ë°ì´í„°ë¥¼ ë§ì´ ë³´ë‚´ê³ , 2ë²ˆì€ ì•ˆ ì¢‹ìœ¼ë‹ˆ ì¡°ê¸ˆë§Œ ë³´ë‚´"ë¼ê³  ê²°ì •í•˜ëŠ” ë‘ë‡Œê°€ í•„ìš”í•©ë‹ˆë‹¤.
* íŒ¨í‚· ì¬ì „ì†¡ (HARQ): RXì—ì„œ ì—ëŸ¬ê°€ ë‚¬ì„ ë•Œ "ë‹¤ì‹œ ë³´ë‚´ì¤˜!"ë¼ê³  TXì— ìš”ì²­í•˜ê³ , TXê°€ ê·¸ê±¸ ê¸°ì–µí–ˆë‹¤ê°€ ë‹¤ì‹œ ë³´ë‚´ì£¼ëŠ” ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
* ìƒìœ„ ë ˆì´ì–´ ì—°ê²°: ì‹¤ì œ ì¸í„°ë„· ë°ì´í„°(YouTube ì˜ìƒ ë“±)ë¥¼ ê°€ì ¸ì™€ì„œ ë¬´ì„  ì‹ í˜¸ì— ì‹¤ìœ¼ë ¤ë©´ IP íŒ¨í‚·ì„ ë¬´ì„  í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³„ì¸µì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤.

ìš”ì•½ ë° ì œì•ˆ
* ë‹¨ìˆœ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ì´ ëª©í‘œë¼ë©´? â†’ L2/L3ë¥¼ ê³¼ê°íˆ ë¹¼ê³  TX-RXë¥¼ ì§ì ‘ ì—°ê²°í•˜ì„¸ìš”.
* ì „ì²´ ê¸°ì§€êµ­ ì•„í‚¤í…ì²˜(O-RAN) ì‹œë®¬ë ˆì´ì…˜ì´ ëª©í‘œë¼ë©´? â†’ L2/L3ë¥¼ ìœ ì§€í•˜ì„¸ìš”.

O-RAN(Open Radio Access Network, ê°œë°©í˜• ë¬´ì„  ì ‘ì†ë§)ì€ ê¸°ì§€êµ­ì„ êµ¬ì„±í•˜ëŠ” í•˜ë“œì›¨ì–´ì™€ ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ë¶„ë¦¬í•˜ê³ , ê·¸ ì‚¬ì´ì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ í‘œì¤€í™”í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ì œì¡°ì‚¬ì˜ ì¥ë¹„ê°€ ì—°ë™ë  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. O-RAN Allianceê°€ ì´ í‘œì¤€í™”ë¥¼ ì£¼ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤
